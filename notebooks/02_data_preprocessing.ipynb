{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d75a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üöÄ EXOHABITAI ‚Äî LEVEL-500 DATA PREPROCESSING PIPELINE\n",
    "# SINGLE CELL ‚Äî PRODUCTION READY\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "print(\"üöÄ ExoHabitAI Data Preprocessing Started\")\n",
    "\n",
    "# ============================================================\n",
    "# üåå AUTO PROJECT ROOT DETECTION\n",
    "# ============================================================\n",
    "\n",
    "CURRENT_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.dirname(CURRENT_DIR)\n",
    "\n",
    "INPUT_PATH = os.path.join(\n",
    "    PROJECT_ROOT,\n",
    "    \"data\",\n",
    "    \"processed\",\n",
    "    \"feature_engineered_exoplanets.csv\"\n",
    ")\n",
    "\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_PATH = os.path.join(\n",
    "    OUTPUT_DIR,\n",
    "    \"model_ready_exoplanets.csv\"\n",
    ")\n",
    "\n",
    "print(\"üìÇ Input:\", INPUT_PATH)\n",
    "\n",
    "# ============================================================\n",
    "# 1Ô∏è‚É£ LOAD DATASET\n",
    "# ============================================================\n",
    "\n",
    "if not os.path.exists(INPUT_PATH):\n",
    "    raise FileNotFoundError(f\"Missing feature engineered dataset:\\n{INPUT_PATH}\")\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "print(\"\\n‚úÖ Dataset Loaded\")\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 2Ô∏è‚É£ SELECT MODEL FEATURES (BACKEND COMPATIBLE)\n",
    "# ============================================================\n",
    "\n",
    "model_features = [\n",
    "    \"pl_rade\",\n",
    "    \"pl_eqt\",\n",
    "    \"pl_orbper\",\n",
    "    \"st_teff\",\n",
    "    \"st_mass\",\n",
    "    \"st_rad\",\n",
    "    \"HSI\",\n",
    "    \"SCI\"\n",
    "]\n",
    "\n",
    "existing_features = [c for c in model_features if c in df.columns]\n",
    "\n",
    "print(\"\\nüß† Features Found:\", existing_features)\n",
    "\n",
    "df_model = df[existing_features + [\"habitability\"]].copy()\n",
    "\n",
    "# ============================================================\n",
    "# 3Ô∏è‚É£ HANDLE MISSING VALUES (SCIENCE SAFE)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüßπ Filling Missing Values...\")\n",
    "\n",
    "numeric_cols = df_model.select_dtypes(include=np.number).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    median_val = df_model[col].median()\n",
    "    df_model[col] = df_model[col].fillna(median_val)\n",
    "\n",
    "# ============================================================\n",
    "# 4Ô∏è‚É£ OUTLIER CLIPPING (NASA SAFE)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüõ∞Ô∏è Clipping extreme outliers...\")\n",
    "\n",
    "for col in existing_features:\n",
    "    q1 = df_model[col].quantile(0.01)\n",
    "    q99 = df_model[col].quantile(0.99)\n",
    "    df_model[col] = df_model[col].clip(q1, q99)\n",
    "\n",
    "# ============================================================\n",
    "# 5Ô∏è‚É£ FEATURE SCALING (NEURAL READY)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Scaling Features...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_model[existing_features] = scaler.fit_transform(\n",
    "    df_model[existing_features]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 6Ô∏è‚É£ SAVE SCALER (VERY IMPORTANT FOR BACKEND)\n",
    "# ============================================================\n",
    "\n",
    "import joblib\n",
    "\n",
    "SCALER_PATH = os.path.join(PROJECT_ROOT, \"backend\", \"models\", \"scaler.pkl\")\n",
    "os.makedirs(os.path.dirname(SCALER_PATH), exist_ok=True)\n",
    "\n",
    "joblib.dump(scaler, SCALER_PATH)\n",
    "\n",
    "print(\"üíæ Scaler Saved:\", SCALER_PATH)\n",
    "\n",
    "# ============================================================\n",
    "# 7Ô∏è‚É£ SAVE FINAL DATASET\n",
    "# ============================================================\n",
    "\n",
    "df_model.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"\\nüíæ Model Ready Dataset Saved:\", OUTPUT_PATH)\n",
    "\n",
    "# ============================================================\n",
    "# 8Ô∏è‚É£ QUICK VISUAL CHECK\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "df_model[\"HSI\"].hist(bins=40)\n",
    "plt.title(\"Scaled HSI Distribution\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "df_model[\"SCI\"].hist(bins=40)\n",
    "plt.title(\"Scaled SCI Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# üéâ LEVEL-500 PREPROCESSING COMPLETE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüöÄ LEVEL-500 PREPROCESSING COMPLETE ‚Äî READY FOR TRAINING\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
